{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830e1c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tibia/miniconda3/envs/libmtl/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befef0e6",
   "metadata": {},
   "source": [
    "# Préparation du dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f8b0b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db088ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEG_LABEL_COLS = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "SEG_DIR = '/home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI'\n",
    "CLASSIFICATION_DATA_DIR = '/home/tibia/Projet_Hemorragie/MBH_label_case'\n",
    "SAVE_DIR = \"/home/tibia/Projet_Hemorragie/MBH_multitask_libMTL/saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "# ======================\n",
    "# DATA PREPARATION\n",
    "# ======================\n",
    "def get_segmentation_data(split=\"train\"):\n",
    "    img_dir = Path(SEG_DIR) / split / \"img\"\n",
    "    seg_dir = Path(SEG_DIR) / split / \"seg\"\n",
    "    \n",
    "    images = sorted(img_dir.glob(\"*.nii.gz\"))\n",
    "    labels = sorted(seg_dir.glob(\"*.nii.gz\"))\n",
    "    \n",
    "    assert len(images) == len(labels), \"Mismatch between image and label counts\"\n",
    "\n",
    "    data = []\n",
    "    for img, lbl in zip(images, labels):\n",
    "        data.append({\n",
    "            \"image\": str(img),\n",
    "            \"label\": str(lbl),\n",
    "        })\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_classification_data(split=\"train\"):\n",
    "    csv_path = Path(CLASSIFICATION_DATA_DIR) / \"splits\" / f\"{split}_split.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    nii_dir = Path(CLASSIFICATION_DATA_DIR)\n",
    "    label_cols = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "    \n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        image_path = str(nii_dir / f\"{row['patientID_studyID']}.nii.gz\")\n",
    "        label = np.array([row[col] for col in label_cols], dtype=np.float32)\n",
    "        \n",
    "        data.append({\n",
    "            \"image\": image_path,\n",
    "            \"label\": label\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de batches dans le DataLoader de segmentation (train) : 77\n",
      "Nombre de batches dans le DataLoader de classification (train) : 637\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from monai.data import DataLoader, PersistentDataset, CacheDataset\n",
    "import torch\n",
    "import os\n",
    "import monai.transforms as T\n",
    "import random\n",
    "\n",
    "batch_size = 2\n",
    "seg_train_data=get_segmentation_data(\"train\")\n",
    "cls_train_data=get_classification_data(\"train\")\n",
    "\n",
    "#val à faire plus tard\n",
    "\n",
    "# Transforms à appliquer après\n",
    "#train_transforms, val_transforms = T_mtsk.TaskBasedTransform_V2(keys=[\"image\", \"label\"]), T_mtsk.TaskBasedValTransform_V2(keys=[\"image\", \"label\"])\n",
    "    \n",
    "seg_transforms = T.Compose([\n",
    "            T.LoadImaged(keys=[\"image\", \"label\"], image_only=True),\n",
    "            T.EnsureChannelFirstd(keys=[\"image\", \"label\"]) ])\n",
    "\n",
    "cls_transforms = T.Compose([  \n",
    "            T.LoadImaged(keys=[\"image\"], image_only=True),\n",
    "            T.EnsureChannelFirstd(keys=[\"image\"]) ])  # Pas de label à charger pour la classification ici\n",
    "\n",
    "    # Datasets\n",
    "seg_train_dataset = PersistentDataset(\n",
    "        seg_train_data, \n",
    "        transform=seg_transforms,\n",
    "        cache_dir=os.path.join(SAVE_DIR, \"cache_train\")\n",
    "    )\n",
    "\n",
    "cls_train_dataset = PersistentDataset(\n",
    "        cls_train_data,\n",
    "        transform=cls_transforms,\n",
    "        cache_dir=os.path.join(SAVE_DIR, \"cache_train\"))\n",
    "    \n",
    "#Val dataset à faire plus tard\n",
    "\n",
    "# DataLoaders\n",
    "seg_train_loader = DataLoader(\n",
    "        seg_train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=8,\n",
    "        persistent_workers=True,\n",
    ")\n",
    "\n",
    "cls_train_loader = DataLoader(\n",
    "        cls_train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=8,\n",
    "        persistent_workers=True,\n",
    ")\n",
    "  \n",
    "train_dataloaders = {'seg': seg_train_loader,\n",
    "                     'cls': cls_train_loader\n",
    "                     }\n",
    "\n",
    "print(f\"Nombre de batches dans le DataLoader de segmentation (train) : {len(train_dataloaders['seg'])}\")\n",
    "print(f\"Nombre de batches dans le DataLoader de classification (train) : {len(train_dataloaders['cls'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcecf876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chemin '/home/tibia/Projet_Hemorragie/LibMTL/LibMTL' a été ajouté à sys.path.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Définir le chemin vers le répertoire 'LibMTL' de la bibliothèque\n",
    "# On utilise os.path.abspath et os.path.join pour assurer la bonne formation du chemin\n",
    "# Dans votre cas, le chemin est : /home/tibia/Projet_Hemorragie/LibMTL/LibMTL/LibMTL\n",
    "libmtl_path = '/home/tibia/Projet_Hemorragie/LibMTL/LibMTL'\n",
    "\n",
    "# Vérifiez si le chemin est déjà dans sys.path pour éviter la duplication\n",
    "if libmtl_path not in sys.path:\n",
    "    # Ajouter le chemin au sys.path\n",
    "    sys.path.insert(0, libmtl_path)\n",
    "    print(f\"Le chemin '{libmtl_path}' a été ajouté à sys.path.\")\n",
    "else:\n",
    "    print(f\"Le chemin '{libmtl_path}' est déjà dans sys.path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086765bc",
   "metadata": {},
   "source": [
    "## Préparation dictionnaire de tâche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "from LibMTL.metrics import AbsMetric\n",
    "from monai.metrics import DiceMetric,DiceHelper\n",
    "from monai.utils import MetricReduction, deprecated_arg\n",
    "        \n",
    "class DiceMetricAdapter(AbsMetric):\n",
    "    \"\"\"\n",
    "    Cet adaptateur implémente AbsMetric pour calculer le Dice Score correctement.\n",
    "    \n",
    "    - `update_fun` utilise DiceHelper pour obtenir les scores bruts (B, C) \n",
    "      et les stocke dans `self.record`.\n",
    "    - `score_fun` agrège tous les scores de `self.record` et calcule \n",
    "      la moyenne finale (le \"score des totaux\" émulé).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, include_background=False):\n",
    "        # Initialise self.record et self.bs\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.include_background = include_background\n",
    "        \n",
    "        # On utilise DiceHelper comme \"calculateur\" ponctuel.\n",
    "        # On lui demande de NE PAS faire de réduction (reduction=\"none\")\n",
    "        # car on veut stocker les scores bruts (Batch, Classes).\n",
    "        self.dice_helper = DiceHelper(\n",
    "            include_background=include_background,\n",
    "            num_classes=num_classes,\n",
    "            reduction=MetricReduction.NONE,\n",
    "            ignore_empty=True,  # Important : ignore les cas où le GT est vide\n",
    "            apply_argmax=False  # On le fera nous-mêmes dans update_fun\n",
    "        )\n",
    "\n",
    "    def update_fun(self, pred, gt):\n",
    "        \"\"\"\n",
    "        Appelé à chaque batch. Calcule les scores (B, C) et les stocke.\n",
    "        \n",
    "        Args:\n",
    "            pred (torch.Tensor): Prédictions (logits) de forme (B, C, H, W, D)\n",
    "            gt (torch.Tensor): Vérité terrain (labels) de forme (B, 1, H, W, D)\n",
    "        \"\"\"\n",
    "        # 1. Convertir les logits en labels\n",
    "        # DiceHelper attend des labels, pas des logits\n",
    "        pred_labels = torch.argmax(pred, dim=1, keepdim=True)\n",
    "        \n",
    "        # 2. Calculer les scores Dice pour ce batch\n",
    "        # Le résultat est un tenseur de (B, num_classes_calculées)\n",
    "        # ex: (B, 5) si num_classes=6 et include_background=False\n",
    "        batch_dice_scores,_ = self.dice_helper(pred_labels, gt)\n",
    "        \n",
    "        # 3. Stocker ce tenseur dans notre \"record\"\n",
    "        self.record.append(batch_dice_scores)\n",
    "        \n",
    "        # 4. Stocker la taille du batch (comme le fait AbsMetric)\n",
    "        self.bs.append(pred.shape[0])\n",
    "\n",
    "    def score_fun(self):\n",
    "        \"\"\"\n",
    "        Appelé à la fin de l'époque. Agrège les scores et calcule la moyenne. Peut etre à modifier pour le loggage de chaque dice\n",
    "        \"\"\"\n",
    "        if not self.record:\n",
    "            # Retourne un score pour chaque classe, mis à 0\n",
    "            num_expected_classes = self.num_classes - (1 if not self.include_background else 0)\n",
    "            return torch.zeros(num_expected_classes)\n",
    "            \n",
    "        # 1. Rassembler tous les tenseurs de (B, C) en un seul\n",
    "        # grand tenseur de (Total_B, C)\n",
    "        all_scores = torch.cat(self.record, dim=0)\n",
    "        \n",
    "        # 2. Calculer la moyenne sur la dimension des batches (dim=0)\n",
    "        # On utilise nanmean pour ignorer les NaN (cas des GT vides)\n",
    "        # C'est la façon correcte d'agréger le Dice.\n",
    "        mean_scores_per_class = torch.nanmean(all_scores, dim=0)\n",
    "        \n",
    "        # `score_fun` est censé retourner une \"liste\", mais un tenseur\n",
    "        # est plus utile. On retourne la moyenne par classe.\n",
    "        return mean_scores_per_class\n",
    "    \n",
    "    # La méthode reinit() est héritée de AbsMetric et fonctionne parfaitement\n",
    "    # car elle vide self.record et self.bs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from monai.networks import nets as monai_nets\n",
    "\n",
    "model = monai_nets.UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=6,\n",
    "    channels=(32, 64, 128, 256, 320, 320),\n",
    "    strides=(2, 2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f163b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels Shape: torch.Size([2, 1, 32, 32, 16])\n",
      "Predicted Labels after Softmax Shape: torch.Size([2, 6, 32, 32, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "B = 2    # Batch Size (nombre d'échantillons)\n",
    "C = 6    # Nombre de Classes (+ background)\n",
    "H = 32   # Hauteur (Height)\n",
    "W = 32   # Largeur (Width)\n",
    "D = 16   # Profondeur (Depth)\n",
    "\n",
    "\n",
    "pred = torch.rand(B, C, H, W, D) \n",
    "pred_labels = torch.argmax(pred, dim=1, keepdim=True)\n",
    "pred_lables_2= torch.nn.Softmax(dim=1)(pred)\n",
    "\n",
    "print(\"Predicted Labels Shape:\", pred_labels.shape)  # Devrait afficher (2, 1, 32, 32, 16)\n",
    "print(\"Predicted Labels after Softmax Shape:\", pred_lables_2.shape)  # Devrait afficher (2, 6, 32, 32, 16)\n",
    "# --- Tenseur de Vérité Terrain (Labels) ---\n",
    "# Forme désirée : (B, 1, H, W, D) -> (2, 1, 32, 32, 16)\n",
    "# Utilisation de torch.randint pour simuler des labels (entiers de 0 à C-1)\n",
    "# Les labels doivent être des entiers et non des flottants.\n",
    "gt = torch.randint(low=0, high=C, size=(B, 1, H, W, D))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69406ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "# Ponderer ensuite pa classe avec WeightSampler\n",
    "\n",
    "from LibMTL.loss import AbsLoss\n",
    "import torch\n",
    "from monai.losses import DiceCELoss\n",
    "\n",
    "class ClassificationLossWrapper(AbsLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def compute_loss(self, pred, gt):\n",
    "        return self.loss_fn(pred, gt.float())\n",
    "    \n",
    "class SegmentationLossWrapper(AbsLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fn = DiceCELoss(\n",
    "            include_background=False,\n",
    "            to_onehot_y=True,\n",
    "            softmax=True\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, pred, gt):\n",
    "        return self.loss_fn(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/miniconda3/envs/libmtl/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /store/home/tibia/miniconda3/envs/libmtl/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/tibia/miniconda3/envs/libmtl/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /store/home/tibia/miniconda3/envs/libmtl/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "from LibMTL.metrics import AbsMetric\n",
    "import torch\n",
    "\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    "\n",
    "class MultiLabelAUCMetric(AbsMetric):\n",
    "    def __init__(self, num_labels=6):\n",
    "        super().__init__()\n",
    "        self.metric = MultilabelAUROC(num_labels=num_labels, average=None)   # par classe\n",
    "        self.metric_mean = MultilabelAUROC(num_labels=num_labels, average=\"macro\")  # moyenne\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def update_fun(self, pred, gt):\n",
    "        # pred = logits -> transform needed\n",
    "        pred = torch.sigmoid(pred)\n",
    "        self.metric.update(pred.detach().cpu(), gt.detach().cpu())\n",
    "        self.metric_mean.update(pred.detach().cpu(), gt.detach().cpu())\n",
    "\n",
    "    def score_fun(self):\n",
    "        per_class = self.metric.compute().tolist()\n",
    "        mean_auc = self.metric_mean.compute().item()\n",
    "        return per_class + [mean_auc]\n",
    "\n",
    "    def reinit(self):\n",
    "        super().reinit()\n",
    "        self.metric.reset()\n",
    "        self.metric_mean.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire de tâches\n",
    "task_dict = {\n",
    "    'classification': {\n",
    "        'loss_fn': ClassificationLossWrapper(),\n",
    "        'metrics_fn': MultiLabelAUCMetric(num_labels=6),\n",
    "        'metrics': ['AUROC'],\n",
    "        'weight': [1.0]\n",
    "    },\n",
    "    'segmentation': {\n",
    "        'loss_fn': SegmentationLossWrapper(),\n",
    "        'metrics_fn': DiceMetricAdapter(num_classes=6, include_background=False),\n",
    "        'metrics': ['Dice_per_class'],\n",
    "        'weight': [1.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0af08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_dict défini avec succès :\n",
      "{'classification': {'loss_fn': BCEWithLogitsLoss(), 'metrics_fn': MultilabelAUROC(), 'metrics': ['AUROC'], 'weight': [1.0]}, 'segmentation': {'loss_fn': DiceCELoss(\n",
      "  (dice): DiceLoss()\n",
      "  (cross_entropy): CrossEntropyLoss()\n",
      "  (binary_cross_entropy): BCEWithLogitsLoss()\n",
      "), 'metrics_fn': <monai.metrics.meandice.DiceHelper object at 0x7f3e711b1190>, 'metrics': ['Dice_per_class'], 'weight': [1.0]}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceHelper\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    "\n",
    "# Background + 5 classes\n",
    "NUM_LABELS_CLASSIF = 6 \n",
    "NUM_CLASSES_SEG = 6 \n",
    "\n",
    "#Fonctions de pertes\n",
    "loss_classif = torch.nn.BCEWithLogitsLoss()\n",
    "loss_seg = DiceCELoss(include_background=False, to_onehot_y=True, softmax=True)\n",
    "\n",
    "#Métriques\n",
    "cls_auc = MultilabelAUROC(num_labels=NUM_LABELS_CLASSIF, average=None)\n",
    "cls_mean_auc = MultilabelAUROC(num_labels=NUM_LABELS_CLASSIF)\n",
    "\n",
    "dice_metric = DiceHelper(\n",
    "            include_background=False,\n",
    "            softmax=True,\n",
    "            num_classes=6,\n",
    "            reduction='none'\n",
    "        )\n",
    "\n",
    "\n",
    "# --- 4. Construire le dictionnaire final ---\n",
    "\n",
    "task_dict = {\n",
    "    'classification': {\n",
    "        'loss_fn': loss_classif,\n",
    "        'metrics_fn': cls_auc,\n",
    "        'metrics': ['AUROC'],\n",
    "        'weight': [1.0]             \n",
    "    },\n",
    "    'segmentation': {\n",
    "        'loss_fn': loss_seg,\n",
    "        'metrics_fn': dice_metric,\n",
    "        'metrics': ['Dice_per_class'],   \n",
    "        'weight': [1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"task_dict défini avec succès :\")\n",
    "print(task_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.networks.nets.basic_unet import TwoConv, Down, UpCat\n",
    "\n",
    "\n",
    "class HemorrhageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe contient la partie descendante (encodeur) du U-Net.\n",
    "    Elle est partagée par les deux tâches.\n",
    "    Son forward pass retourne une liste de toutes les feature maps\n",
    "    nécessaires pour les skip connections du décodeur de segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        features: Sequence[int] = (32, 32, 64, 128, 256, 32),\n",
    "        act: str | tuple = (\"LeakyReLU\", {\"negative_slope\": 0.1, \"inplace\": True}),\n",
    "        norm: str | tuple = (\"instance\", {\"affine\": True}),\n",
    "        bias: bool = True,\n",
    "        dropout: float | tuple = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Assure que 'features' a la bonne longueur\n",
    "        self.fea = nn.Parameter(torch.tensor(features), requires_grad=False)\n",
    "        \n",
    "        self.conv_0 = TwoConv(spatial_dims, in_channels, self.fea[0], act, norm, bias, dropout)\n",
    "        self.down_1 = Down(spatial_dims, self.fea[0], self.fea[1], act, norm, bias, dropout)\n",
    "        self.down_2 = Down(spatial_dims, self.fea[1], self.fea[2], act, norm, bias, dropout)\n",
    "        self.down_3 = Down(spatial_dims, self.fea[2], self.fea[3], act, norm, bias, dropout)\n",
    "        self.down_4 = Down(spatial_dims, self.fea[3], self.fea[4], act, norm, bias, dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> list[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Le forward pass de l'encodeur.\n",
    "        Retourne une liste contenant le bottleneck (x4) et toutes les\n",
    "        sorties intermédiaires pour les skip connections.\n",
    "        \"\"\"\n",
    "        x0 = self.conv_0(x)\n",
    "        x1 = self.down_1(x0)\n",
    "        x2 = self.down_2(x1)\n",
    "        x3 = self.down_3(x2)\n",
    "        x4 = self.down_4(x3)  # C'est le bottleneck (la représentation partagée)\n",
    "        \n",
    "        return [x4, x3, x2, x1, x0]\n",
    "\n",
    "# ========================================================================\n",
    "# 2. LES DÉCODEURS (Les têtes spécifiques à chaque tâche)\n",
    "# ========================================================================\n",
    "\n",
    "class SegmentationDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Le décodeur pour la tâche de segmentation.\n",
    "    Il prend la liste de features de l'encodeur et reconstruit le masque.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int = 3,\n",
    "        out_channels: int = 6,\n",
    "        features: Sequence[int] = (32, 32, 64, 128, 256, 32),\n",
    "        act: str | tuple = (\"LeakyReLU\", {\"negative_slope\": 0.1, \"inplace\": True}),\n",
    "        norm: str | tuple = (\"instance\", {\"affine\": True}),\n",
    "        bias: bool = True,\n",
    "        dropout: float | tuple = 0.0,\n",
    "        upsample: str = \"deconv\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        fea = nn.Parameter(torch.tensor(features), requires_grad=False)\n",
    "        \n",
    "        self.upcat_4 = UpCat(spatial_dims, fea[4], fea[3], fea[3], act, norm, bias, dropout, upsample)\n",
    "        self.upcat_3 = UpCat(spatial_dims, fea[3], fea[2], fea[2], act, norm, bias, dropout, upsample)\n",
    "        self.upcat_2 = UpCat(spatial_dims, fea[2], fea[1], fea[1], act, norm, bias, dropout, upsample)\n",
    "        self.upcat_1 = UpCat(spatial_dims, fea[1], fea[0], fea[5], act, norm, bias, dropout, upsample, halves=False)\n",
    "        self.final_conv = nn.Conv3d(fea[5], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, enc_out: list[torch.Tensor]) -> torch.Tensor:\n",
    "        # On récupère les tenseurs de la liste fournie par l'encodeur\n",
    "        x4, x3, x2, x1, x0 = enc_out\n",
    "        \n",
    "        u4 = self.upcat_4(x4, x3)\n",
    "        u3 = self.upcat_3(u4, x2)\n",
    "        u2 = self.upcat_2(u3, x1)\n",
    "        u1 = self.upcat_1(u2, x0)\n",
    "        \n",
    "        return self.final_conv(u1)\n",
    "\n",
    "class ClassificationDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Le décodeur pour la tâche de classification.\n",
    "    Il prend la liste de features de l'encodeur mais n'utilise que le\n",
    "    bottleneck (x4) pour prédire les classes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,  # Doit correspondre à features[4] de l'encodeur\n",
    "        num_cls_classes: int = 6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Tête de classification, exactement comme avant\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((4, 4, 4)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features * 4 * 4 * 4, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_cls_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, enc_out: list[torch.Tensor]) -> torch.Tensor:\n",
    "        # On ne prend que le bottleneck (le premier élément de la liste)\n",
    "        x4 = enc_out[0]\n",
    "        \n",
    "        # Toute la logique d'agrégation de patches a disparu !\n",
    "        # On passe directement les features à la tête de classification.\n",
    "        return self.cls_head(x4)\n",
    "\n",
    "# ========================================================================\n",
    "# 3. ASSEMBLAGE FINAL POUR LibMTL\n",
    "# ========================================================================\n",
    "\n",
    "# Définis tes paramètres\n",
    "task_name = [\"segmentation\", \"classification\"]\n",
    "features = (32, 32, 64, 128, 256, 32)\n",
    "\n",
    "# Crée une instance de l'encodeur partagé\n",
    "encoder = HemorrhageEncoder(features=features)\n",
    "\n",
    "# Crée un dictionnaire de décodeurs\n",
    "decoders = nn.ModuleDict({\n",
    "    'segmentation': SegmentationDecoder(\n",
    "        out_channels=6, # 6 classes de segmentation\n",
    "        features=features\n",
    "    ),\n",
    "    'classification': ClassificationDecoder(\n",
    "        in_features=features[4], # La taille du bottleneck (256)\n",
    "        num_cls_classes=6 # 6 classes de classification\n",
    "    )\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be35e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LibMTL.architecture import SharedBottom\n",
    "from LibMTL.weighting import Equal\n",
    "import wandb\n",
    "\n",
    "config_l = dict(\n",
    "    sharing_type=\"hard\",   # \"soft\" ou \"fine_tune\"\n",
    "    model=\"BasicUNetWithClassification\",\n",
    "    loss_weighting=\"none\",\n",
    "    dataset_size=\"balanced\",  # \"full\" ou \"balanced\" ou \"optimized\"\n",
    "    batch_size=2,\n",
    "    learning_rate=1e-3,\n",
    "    optimizer=\"sgd\",\n",
    "    seed=42\n",
    ")\n",
    "torch.cuda.set_device(0)\n",
    "# Génération automatique de tags à partir de config\n",
    "tags = [f\"{k}:{v}\" for k, v in config_l.items() if k in [\"sharing_type\", \"optimizer\", \"model\", \"loss_weighting\"]]\n",
    "\n",
    "# ⬇️ CHANGEMENT : Initialisation manuelle de wandb\n",
    "# Au lieu de : wandb_logger = WandbLogger(...)\n",
    "wandb.init(\n",
    "    project=\"hemorrhage_multitask_test\",\n",
    "    group=\"noponderation\",\n",
    "    tags=tags,\n",
    "    config=config_l,\n",
    "    name=\"multitask_unet3d_libMTL\"\n",
    ")\n",
    "\n",
    "# Paramètres optim & scheduler\n",
    "optim_param = {\n",
    "    'method': 'SGD', \n",
    "    'lr': 1e-3, \n",
    "    'weight_decay': 3e-5,  # 0.00003 est égal à 3e-5\n",
    "    'momentum': 0.99, \n",
    "    'nesterov': True\n",
    "}\n",
    "\n",
    "scheduler_param = {\n",
    "    'method': 'LinearScheduleWithWarmup',  # Correspond à get_linear_schedule_with_warmup\n",
    "    'num_warmup_steps': 0, \n",
    "    'num_training_steps': \"VOTRE_TOTAL_STEPS\" # Ceci correspond à self.num_steps\n",
    "}\n",
    "\n",
    "# 3️ Méthodes multitâches\n",
    "from LibMTL.architecture import HPS\n",
    "from LibMTL.weighting import GradNorm\n",
    "\n",
    "# 4️ Instanciation du Trainer\n",
    "from LibMTL import Trainer\n",
    "\n",
    "hemorrhage_trainer = Trainer(\n",
    "    task_dict=task_dict,\n",
    "    weighting=None,\n",
    "    architecture=HPS,\n",
    "    encoder_class=HemorrhageEncoder,\n",
    "    decoders=decoders,\n",
    "    rep_grad=False,\n",
    "    multi_input=True,\n",
    "    optim_param=optim_param,\n",
    "    scheduler_param=scheduler_param,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# 5️⃣ Entraînement\n",
    "hemorrhage_trainer.train(train_dataloaders, num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libmtl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
